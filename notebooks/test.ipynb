{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c5cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from explainn import tools\n",
    "from explainn import networks\n",
    "from explainn import train\n",
    "from explainn import test\n",
    "from explainn import interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a407a3b-9f8b-4f09-8c65-8f5228b92c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51a5603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "h5_file = \"../data/test/tf_peaks_TEST_sparse_Remap.h5\"\n",
    "if not os.path.exists(h5_file):\n",
    "    os.system(f\"zless {h5_file}.gz > {h5_file}\")\n",
    "\n",
    "dataloaders, target_labels, train_out = tools.load_datas(h5_file,\n",
    "                                                         batch_size,\n",
    "                                                         0,\n",
    "                                                         True)\n",
    "\n",
    "target_labels = [i.decode(\"utf-8\") for i in target_labels]\n",
    "\n",
    "num_cnns = 100\n",
    "input_length = 200\n",
    "num_classes = len(target_labels)\n",
    "filter_size = 19\n",
    "\n",
    "\n",
    "model = networks.ExplaiNN(num_cnns, input_length, num_classes, filter_size).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffc80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/fs-2/scale/OrionStore/Home/asfj/ExplaiNN/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bee6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Current Train Loss: 0.60254, Current Val Loss: 0.55127\n",
      "Epoch [2], Current Train Loss: 0.53691, Current Val Loss: 0.53363\n",
      "Epoch [3], Current Train Loss: 0.52337, Current Val Loss: 0.53137\n",
      "Epoch [4], Current Train Loss: 0.51611, Current Val Loss: 0.53109\n",
      "Epoch [5], Current Train Loss: 0.50973, Current Val Loss: 0.53334\n",
      "Epoch [6], Current Train Loss: 0.50340, Current Val Loss: 0.53686\n"
     ]
    }
   ],
   "source": [
    "weights_folder = \"../data/test/weights\"\n",
    "if not os.path.exists(weights_folder):\n",
    "    os.makedirs(weights_folder)\n",
    "\n",
    "model, train_error, test_error = train.train_explainn(dataloaders[\"train\"],\n",
    "                                                      dataloaders[\"valid\"],\n",
    "                                                      model,\n",
    "                                                      device,\n",
    "                                                      criterion,\n",
    "                                                      optimizer,\n",
    "                                                      num_epochs,\n",
    "                                                      weights_folder,\n",
    "                                                      name_ind=\"\",\n",
    "                                                      verbose=True,\n",
    "                                                      trim_weights=False,\n",
    "                                                      checkpoint=0,\n",
    "                                                      patience=0)\n",
    "\n",
    "tools.showPlot(train_error, test_error, \"Loss trend\", \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38296f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAX': 0.8210951541889224, 'FOXA1': 0.8962736970641367, 'JUND': 0.7473675125894179}\n",
      "{'MAX': 0.7987115942028986, 'FOXA1': 0.811336302537931, 'JUND': 0.744590973993679}\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"{weights_folder}/{os.listdir(weights_folder)[0]}\"))\n",
    "\n",
    "labels_E, outputs_E = test.run_test(model, dataloaders[\"test\"], device)\n",
    "pr_rec = average_precision_score(labels_E, outputs_E)\n",
    "\n",
    "no_skill_probs = [0 for _ in range(len(labels_E[:, 0]))]\n",
    "ns_fpr, ns_tpr, _ = metrics.roc_curve(labels_E[:, 0], no_skill_probs)\n",
    "\n",
    "roc_aucs = {}\n",
    "raw_aucs = {}\n",
    "roc_prcs = {}\n",
    "raw_prcs = {}\n",
    "for i in range(len(target_labels)):\n",
    "    nn_fpr, nn_tpr, threshold = metrics.roc_curve(labels_E[:, i], outputs_E[:, i])\n",
    "    roc_auc_nn = metrics.auc(nn_fpr, nn_tpr)\n",
    "\n",
    "    precision_nn, recall_nn, thresholds = metrics.precision_recall_curve(labels_E[:, i], outputs_E[:, i])\n",
    "    pr_auc_nn = metrics.auc(recall_nn, precision_nn)\n",
    "\n",
    "    raw_aucs[target_labels[i]] = nn_fpr, nn_tpr\n",
    "    roc_aucs[target_labels[i]] = roc_auc_nn\n",
    "\n",
    "    raw_prcs[target_labels[i]] = recall_nn, precision_nn\n",
    "    roc_prcs[target_labels[i]] = pr_auc_nn\n",
    "\n",
    "print(roc_prcs)\n",
    "print(roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddcf9482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 354/354 [00:55<00:00,  6.33it/s]\n",
      "100%|████████████████████| 100/100 [01:11<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PWM File as : ../data/test/explainn_filters.meme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset, data_inp, data_out = tools.load_single_data(h5_file,\n",
    "                                                     batch_size,\n",
    "                                                     0,\n",
    "                                                     False)\n",
    "\n",
    "predictions, labels = interpretation.get_explainn_predictions(dataset,\n",
    "                                                              model,\n",
    "                                                              device,\n",
    "                                                              isSigmoid=True)\n",
    "\n",
    "# only well predicted sequences\n",
    "pred_full_round = np.round(predictions)\n",
    "arr_comp = np.equal(pred_full_round, labels)\n",
    "idx = np.argwhere(np.sum(arr_comp, axis=1) == len(target_labels)).squeeze()\n",
    "\n",
    "data_inp = data_inp[idx, :, :]\n",
    "data_out = data_out[idx, :]\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data_inp, data_out)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0)\n",
    "\n",
    "activations = interpretation.get_explainn_unit_activations(data_loader, model, device)\n",
    "pwms = interpretation.get_pwms_explainn(activations, data_inp, filter_size)\n",
    "meme_file = \"../data/test/explainn_filters.meme\"\n",
    "interpretation.pwm_to_meme(pwms, meme_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4bec38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: tomtom: command not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomtom_file = \"../data/test/MAX_JUND_FOXA1_tomtom.tsv\"\n",
    "jaspar_meme = \"../data/JASPAR/JASPAR2020_CORE_vertebrates_non-redundant_pfms_meme.txt\"\n",
    "os.system(f\"tomtom --text {meme_file} {jaspar_meme} > {tomtom_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ae0957",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tomtom_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtomtom_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m filters_with_min_q \u001b[38;5;241m=\u001b[39m tomtom_results\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmin()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq-value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m tomtom_results \u001b[38;5;241m=\u001b[39m tomtom_results[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq-value\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1289\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m   1274\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1275\u001b[0m     dialect,\n\u001b[1;32m   1276\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1286\u001b[0m )\n\u001b[1;32m   1287\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1753\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:79\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/explainn/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:554\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "tomtom_results = pd.read_table(f\"{tomtom_file}\", comment=\"#\")\n",
    "\n",
    "filters_with_min_q = tomtom_results.groupby(\"Query_ID\").min()[\"q-value\"]\n",
    "\n",
    "tomtom_results = tomtom_results[[\"Target_ID\", \"Query_ID\", \"q-value\"]]\n",
    "tomtom_results = tomtom_results[tomtom_results[\"q-value\"]<0.05]\n",
    "\n",
    "jaspar_motifs = {}\n",
    "with open(jaspar_meme) as f:\n",
    "    for line in f:\n",
    "        if \"MOTIF\" in line:\n",
    "            motif = line.strip().split()[-1]\n",
    "            name_m = line.strip().split()[-2]\n",
    "            jaspar_motifs[name_m] = motif\n",
    "\n",
    "filters = tomtom_results[\"Query_ID\"].unique()\n",
    "annotation = {}\n",
    "for f in filters:\n",
    "    t = tomtom_results[tomtom_results[\"Query_ID\"] == f]\n",
    "    target_id = t[\"Target_ID\"]\n",
    "    if len(target_id) > 5:\n",
    "        target_id = target_id[:5]\n",
    "    ann = \"/\".join([jaspar_motifs[i] for i in target_id.values])\n",
    "    annotation[f] = ann\n",
    "\n",
    "annotation = pd.Series(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da353a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.final.weight.detach().cpu().numpy()\n",
    "\n",
    "filters = [\"filter\"+str(i) for i in range(num_cnns)]\n",
    "for i in annotation.keys():\n",
    "    filters[int(i.split(\"filter\")[-1])] = annotation[i]\n",
    "\n",
    "weight_df = pd.DataFrame(weights, index=target_labels, columns=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "# focus on annotated filters only\n",
    "sns.clustermap(weight_df[[i for i in weight_df.columns if not i.startswith(\"filter\")]],\n",
    "               cmap=sns.diverging_palette(145, 10, s=60, as_cmap=True),\n",
    "               row_cluster=False,\n",
    "               figsize=(30, 20),\n",
    "               vmax=0.5,\n",
    "               vmin=-0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6349c56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m unit_outputs \u001b[38;5;241m=\u001b[39m interpretation\u001b[38;5;241m.\u001b[39mget_explainn_unit_outputs(data_loader, model, device)\n\u001b[0;32m----> 3\u001b[0m best_filters \u001b[38;5;241m=\u001b[39m \u001b[43mweight_df\u001b[49m\u001b[38;5;241m.\u001b[39midxmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m best_myc_max_filter \u001b[38;5;241m=\u001b[39m weight_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(best_filters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m unit_importance \u001b[38;5;241m=\u001b[39m interpretation\u001b[38;5;241m.\u001b[39mget_specific_unit_importance(activations,\n\u001b[1;32m      7\u001b[0m                                                               model,\n\u001b[1;32m      8\u001b[0m                                                               unit_outputs,\n\u001b[1;32m      9\u001b[0m                                                               best_myc_max_filter,\n\u001b[1;32m     10\u001b[0m                                                               target_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weight_df' is not defined"
     ]
    }
   ],
   "source": [
    "unit_outputs = interpretation.get_explainn_unit_outputs(data_loader, model, device)\n",
    "\n",
    "best_filters = weight_df.idxmax(axis=\"columns\")\n",
    "best_myc_max_filter = weight_df.columns.get_loc(best_filters[\"MAX\"])\n",
    "\n",
    "unit_importance = interpretation.get_specific_unit_importance(activations,\n",
    "                                                              model,\n",
    "                                                              unit_outputs,\n",
    "                                                              best_myc_max_filter,\n",
    "                                                              target_labels)\n",
    "\n",
    "filter_key = f\"filter{best_myc_max_filter}\"\n",
    "title = annotation[filter_key] if filter_key in annotation.index else filter_key\n",
    "fig, ax = plt.subplots()\n",
    "datas = [filt_dat for filt_dat in unit_importance]\n",
    "ax.boxplot(datas, notch=True, patch_artist=True, boxprops=dict(facecolor=\"#228833\", color=\"#228833\"))\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.title(title)\n",
    "plt.ylabel(\"Unit importance\")\n",
    "plt.xticks(range(1, len(target_labels)+1), target_labels)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057cc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11664a-76be-4d04-9070-e78555f44371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f966f3f-b8c2-44e3-9e83-736ab2361ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainn",
   "language": "python",
   "name": "explainn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
