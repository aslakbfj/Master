{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from lens.models.relu_nn import XReluNN\n",
    "from lens.models.psi_nn import XPsiNetwork\n",
    "from lens.utils.base import set_seed, ClassifierNotTrainedError, IncompatibleClassifierError\n",
    "from lens.utils.metrics import Accuracy, F1Score\n",
    "from lens.models.mu_nn import XMuNN\n",
    "from lens.utils.datasets import ConceptToTaskDataset, ImageToConceptAndTaskDataset\n",
    "from lens.utils.data import get_splits_train_val_test, get_transform\n",
    "from lens.models.concept_extractor.cnn_models import RESNET18\n",
    "from data import CUB200\n",
    "from data.download_cub import download_cub\n",
    "from experiments.CUB_200_2011.concept_extractor_cub import concept_extractor_cub\n",
    "from experiments.CUB_200_2011.adversarial_attack import generate_adversarial_data, single_label_evaluate, \\\n",
    "    create_single_label_dataset\n",
    "\n",
    "results_dir = 'results/cub'\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading CUB data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_root = \"../data/CUB_200_2011/\"\n",
    "dataset_name = CUB200\n",
    "print(dataset_root)\n",
    "if not os.path.isdir(dataset_root):\n",
    "    download_cub(dataset_root)\n",
    "else:\n",
    "    print(\"Dataset already downloaded\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = ConceptToTaskDataset(dataset_root)\n",
    "train_data, val_data, test_data = get_splits_train_val_test(dataset, load=False)\n",
    "class_names = dataset.classes\n",
    "concept_names = dataset.attribute_names\n",
    "print(\"Concept names\", concept_names)\n",
    "n_features = dataset.n_attributes\n",
    "print(\"Number of attributes\", n_features)\n",
    "n_classes = dataset.n_classes\n",
    "print(\"Number of classes\", n_classes)\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define loss, metrics and methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "metric = Accuracy()\n",
    "expl_metric = F1Score()\n",
    "method_list = ['General', 'Psi', 'Relu']\n",
    "print(\"Methods\", method_list)\n",
    "batch_size = 128\n",
    "attack = \"apgd-t\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training black box on images (to predict labels and attributes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bb_model = concept_extractor_cub(dataset_root, result_folder=\"CUB_200_2011\", robust=True,\n",
    "                                 multi_label=True, device=device, cnn_model=RESNET18,\n",
    "                                 seeds=[0], save_predictions=False, show_image=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using outputs of black box model as input and targets for other surrogate models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transform = get_transform(dataset=dataset_name, data_augmentation=True)\n",
    "bb_dataset = ImageToConceptAndTaskDataset(dataset_root, train_transform, dataset_name=dataset_name)\n",
    "train_data_bb, val_data_bb, test_data_bb = get_splits_train_val_test(bb_dataset, load=False)\n",
    "prediction_name = os.path.join(results_dir, \"black_box_predictions.pth\")\n",
    "if os.path.isfile(prediction_name):\n",
    "    outputs_bb, labels_bb = torch.load(prediction_name)\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        outputs_bb, labels_bb = bb_model.predict(bb_dataset, device=device, batch_size=batch_size)\n",
    "    torch.save((outputs_bb, labels_bb), prediction_name)\n",
    "dataset.targets = outputs_bb[:, :n_classes].detach().cpu()\n",
    "dataset.attributes = outputs_bb[:, n_classes:].detach().cpu()\n",
    "print(\"Black Box predictions saved\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attacking model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, _, test_data_bb_sl = create_single_label_dataset(test_data_bb, range(n_classes))\n",
    "bb_accuracy_clean, _ = single_label_evaluate(bb_model, test_data_bb_sl, range(n_classes), device=device)\n",
    "print(\"Main classes accuracy on clean test data\", bb_accuracy_clean)\n",
    "adv_dataset = generate_adversarial_data(bb_model, test_data_bb_sl, dataset_name, attack,\n",
    "                                        result_folder=results_dir, device=device)\n",
    "bb_accuracy_adv, bb_rejection_adv = single_label_evaluate(bb_model, adv_dataset, range(n_classes), device=device)\n",
    "print(\"Main classes accuracy on adv test data\", bb_accuracy_adv)\n",
    "multi_label_test_labels = labels_bb[test_data_bb.indices, :]\n",
    "with torch.no_grad():\n",
    "    adv_multilabel_prediction, _ = bb_model.predict(adv_dataset, batch_size, device=device)\n",
    "accuracy_adv_data_multilabel = bb_model.evaluate(adv_dataset, outputs=adv_multilabel_prediction,\n",
    "                                                 labels=multi_label_test_labels, metric=F1Score())\n",
    "print(\"Multilabel accuracy on adv test data\", accuracy_adv_data_multilabel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting training hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "n_processes = 1\n",
    "timeout = 6 * 60 * 60  # 6 h timeout\n",
    "l_r = 1e-3\n",
    "lr_scheduler = False\n",
    "top_k_explanations = None\n",
    "simplify = True\n",
    "seeds = [*range(0, 5)]\n",
    "print(\"Seeds\", seeds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training explanation methods and testing explanations on the adversarial data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for method in method_list:\n",
    "\n",
    "    methods = []\n",
    "    splits = []\n",
    "    model_accuracies = []\n",
    "    model_explanations = []\n",
    "    rejection_rates = []\n",
    "    bb_accuracy_with_rej_clean = []\n",
    "    bb_accuracy_with_rej_adv = []\n",
    "    bb_rejection_rate_clean = []\n",
    "    bb_rejection_rate_adv = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)\n",
    "        name = os.path.join(results_dir, f\"{method}_{seed}\")\n",
    "\n",
    "        train_data, val_data, test_data = get_splits_train_val_test(dataset, load=False)\n",
    "        x_train = torch.as_tensor(dataset.attributes[train_data.indices])\n",
    "        y_train = torch.as_tensor(dataset.targets[train_data.indices])\n",
    "        x_val = torch.as_tensor(dataset.attributes[val_data.indices])\n",
    "        y_val = torch.as_tensor(dataset.targets[val_data.indices])\n",
    "        x_test = torch.as_tensor(dataset.attributes[test_data.indices])\n",
    "        y_test = torch.as_tensor(dataset.targets[test_data.indices])\n",
    "        print(train_data.indices)\n",
    "\n",
    "        # Setting device\n",
    "        print(f\"Training {name} classifier...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method == 'Psi':\n",
    "            # Network structures\n",
    "            l1_weight = 1e-4\n",
    "            hidden_neurons = [10]\n",
    "            fan_in = 4\n",
    "            model = XPsiNetwork(n_classes, n_features, hidden_neurons, loss,\n",
    "                               l1_weight, name=name, fan_in=fan_in)\n",
    "            try:\n",
    "                model.load(device)\n",
    "                print(f\"Model {name} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                results = model.fit(train_data, val_data, epochs=epochs, l_r=l_r, verbose=True,\n",
    "                                    metric=metric, lr_scheduler=lr_scheduler, device=device, save=True)\n",
    "            outputs, labels = model.predict(test_data, device=device)\n",
    "            accuracy = model.evaluate(test_data, metric=metric, outputs=outputs, labels=labels)\n",
    "            print(\"Test model accuracy\", accuracy)\n",
    "            formulas = []\n",
    "            for i, class_to_explain in enumerate(dataset.classes):\n",
    "                formula = model.get_global_explanation(i, concept_names, simplify=simplify)\n",
    "                formulas.append(formula)\n",
    "\n",
    "        elif method == 'General':\n",
    "            # Network structures\n",
    "            lr_general = l_r\n",
    "            l1_weight = 1e-4\n",
    "            hidden_neurons = [20]\n",
    "            set_seed(seed)\n",
    "            model = XMuNN(n_classes=dataset.n_classes, n_features=n_features, hidden_neurons=hidden_neurons,\n",
    "                               loss=loss, l1_weight=l1_weight, fan_in=10, name=name)\n",
    "            try:\n",
    "                model.load(device)\n",
    "                print(f\"Model {name} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                results = model.fit(train_data, val_data, epochs=epochs, l_r=lr_general, metric=metric,\n",
    "                                    lr_scheduler=lr_scheduler, device=device, save=True, verbose=True)\n",
    "            outputs, labels = model.predict(test_data, device=device)\n",
    "            accuracy = model.evaluate(test_data, metric=F1Score(), outputs=outputs, labels=labels)\n",
    "            print(\"Test model accuracy\", accuracy)\n",
    "            formulas = []\n",
    "            for i, class_to_explain in enumerate(dataset.classes):\n",
    "                formula = model.get_global_explanation(x_train, y_train, i, top_k_explanations, concept_names,\n",
    "                                                       simplify=simplify, metric=expl_metric,\n",
    "                                                       x_val=x_val, y_val=y_val )\n",
    "                formulas.append(formula)\n",
    "\n",
    "        elif method == 'Relu':\n",
    "            # Network structures\n",
    "            l1_weight = 1e-7\n",
    "            hidden_neurons = [300, 200]\n",
    "            print(\"L1 weight\", l1_weight)\n",
    "            print(\"Hidden neurons\", hidden_neurons)\n",
    "            model = XReluNN(n_classes, n_features, hidden_neurons, loss, name=name,\n",
    "                            l1_weight=l1_weight)\n",
    "            try:\n",
    "                model.load(device)\n",
    "                print(f\"Model {name} already trained\")\n",
    "            except (ClassifierNotTrainedError, IncompatibleClassifierError):\n",
    "                results = model.fit(train_data, val_data, epochs=epochs, l_r=l_r, verbose=True,\n",
    "                                    metric=metric, lr_scheduler=lr_scheduler, device=device, save=True)\n",
    "            outputs, labels = model.predict(test_data, device=device)\n",
    "            accuracy = model.evaluate(test_data, metric=metric, outputs=outputs, labels=labels)\n",
    "            print(\"Test model accuracy\", accuracy)\n",
    "            formulas = []\n",
    "            for i, class_to_explain in enumerate(dataset.classes):\n",
    "                formula = model.get_global_explanation(x_train, y_train, i, top_k_explanations, concept_names,\n",
    "                                                       simplify=simplify, metric=expl_metric,\n",
    "                                                       x_val=x_val, y_val=y_val)\n",
    "                formulas.append(formula)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{method} not implemented\")\n",
    "\n",
    "        bb_model.set_explanations(formulas)\n",
    "        bb_model.calc_threshold(val_data_bb, batch_size=batch_size)\n",
    "        bb_accuracy, bb_rejection_rate = single_label_evaluate(bb_model, test_data_bb_sl, range(n_classes),\n",
    "                                                               reject=True, adv=False, device=device)\n",
    "        print(\"Accuracy and rejection on clean data\", bb_accuracy, bb_rejection_rate)\n",
    "        adv_bb_accuracy, bb_adv_rejection_rate = single_label_evaluate(bb_model, adv_dataset, range(n_classes),\n",
    "                                                                       reject=True, adv=True, device=device)\n",
    "        print(\"Accuracy and rejection on adv data\", adv_bb_accuracy, bb_adv_rejection_rate)\n",
    "\n",
    "        methods.append(method)\n",
    "        splits.append(seed)\n",
    "        model_explanations.append(formulas[0])\n",
    "        model_accuracies.append(accuracy)\n",
    "        bb_accuracy_with_rej_clean.append(bb_accuracy)\n",
    "        bb_accuracy_with_rej_adv.append(adv_bb_accuracy)\n",
    "        bb_rejection_rate_clean.append(bb_rejection_rate)\n",
    "        bb_rejection_rate_adv.append(bb_adv_rejection_rate)\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'method': methods,\n",
    "        'split': splits,\n",
    "        'explanation': model_explanations,\n",
    "        'model_accuracy': model_accuracies,\n",
    "        'bb_accuracy_clean': [bb_accuracy_clean] * len(seeds),\n",
    "        'bb_accuracy_adv': [bb_accuracy_adv] * len(seeds),\n",
    "        'bb_accuracy_clean_rej': bb_accuracy_with_rej_clean,\n",
    "        'bb_accuracy_adv_rej': bb_accuracy_with_rej_adv,\n",
    "        'bb_rejection_rate_clean': bb_rejection_rate_clean,\n",
    "        'bb_rejection_rate_adv': bb_rejection_rate_adv,\n",
    "    })\n",
    "    results.to_csv(os.path.join(results_dir, f'adv_results_{method}.csv'))\n",
    "    print(results)\n",
    "\n",
    "results_df = {}\n",
    "summaries = {}\n",
    "for m in method_list:\n",
    "    results_df[m] = pd.read_csv(os.path.join(results_dir, f\"adv_results_{m}.csv\"))\n",
    "\n",
    "results_df = pd.concat([results_df[method] for method in method_list])\n",
    "results_df.to_csv(os.path.join(results_dir, f'adv_results.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}